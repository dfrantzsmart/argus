{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Argus\n",
    "\n",
    "## Summary  \n",
    "\n",
    "The purpose of this project is to classify a growth as either Malignant (\"M\") or Benign (\"B\").   \n",
    "We want to REDUCE the occurance of FALSE NEGATIVES as much as possible.  \n",
    "That is: we want to avoid telling someone they do not have cancer, when in fact they do.   \n",
    "This is the [Jupyter notebook](argusMain.ipynb).  \n",
    "These are the [script functions](argusFuncs.py).   \n",
    "Here is a [powerpoint format](jcosme-project3.pptx) of the presentation, and here is a [pdf format](jcosme-project3.pdf) of the presentation.  \n",
    "\n",
    "### Outline  \n",
    "+ The Data\n",
    "+ Simple Models\n",
    "+ Ensemble Model Idea\n",
    "+ Building Ensemble Model\n",
    "+ Results\n",
    "+ Conclusion\n",
    "\n",
    "## The Data\n",
    "\n",
    "The data contains 569 labeled breast cancer observations  \n",
    "It was obtained from [uci](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin).\n",
    "\n",
    "Each observation contains the following:  \n",
    "+ id\n",
    "+ diagnosis\n",
    "+ radius_mean\n",
    "+ texture_mean\n",
    "+ perimeter_mean\n",
    "+ area_mean\n",
    "+ smoothness_mean\n",
    "+ compactness_mean\n",
    "+ concavity_mean\n",
    "+ concave_points_mean\n",
    "+ symmetry_mean\n",
    "+ fractal_dimension_mean\n",
    "+ radius_se\n",
    "+ texture_se\n",
    "+ perimeter_se\n",
    "+ area_se\n",
    "+ smoothness_se\n",
    "+ compactness_se\n",
    "+ concavity_se\n",
    "+ concave_points_se\n",
    "+ symmetry_se\n",
    "+ fractal_dimension_se\n",
    "+ radius_worst\n",
    "+ texture_worst\n",
    "+ perimeter_worst\n",
    "+ area_worst\n",
    "+ smoothness_worst\n",
    "+ compactness_worst\n",
    "+ concavity_worst\n",
    "+ concave_points_worst\n",
    "+ symmetry_worst\n",
    "+ fractal_dimension_worst\n",
    "\n",
    "We will split the data RANDOMLY into Train (70%), Dev (15%), and Test (15%) segments, as shown:\n",
    "\n",
    "![initial split](images/dataSplit.png)\n",
    "\n",
    "To start, we will work with the Train data set:\n",
    "\n",
    "![train split](images/workingSplit.png)\n",
    "\n",
    "## Simple Models\n",
    "\n",
    "We create 3 simple models (on the Train data set), to use as a basis:\n",
    "+ Logistic Regression\n",
    "+ KMeans\n",
    "+ Neural Net  \n",
    "\n",
    "We make predictions on the Test set and Dev set.   \n",
    "\n",
    "Here are the confusion matrices:\n",
    "\n",
    "![simple model confusion matrix](images/simpleModelResultsCMs.svg)\n",
    "\n",
    "From the above, we can see that the KMeans has the lowest rate of False Positives, but with a disproportionately higher False Negative rate.   \n",
    "We can also see that the Neural Network gives inconsistent False Negative/Positive results.  \n",
    "Of the 3 models, the Logistic Regression seems to be the most stable.\n",
    "\n",
    "## Ensemble Model Idea\n",
    "\n",
    "In a perfect world, our data would look like this, in two dimensions, and be easily linearly separable: \n",
    "\n",
    "![ideal data](images/idealData.png)\n",
    "\n",
    "However, this is not a perfect world, so maybe our data looks more like this:\n",
    "\n",
    "![realistic data](images/realisticData.png)\n",
    "\n",
    "The idea is to group the data into distinct clusters, then build separate models on each cluster, that predict data labels. \n",
    "\n",
    "We have a problem: our data has 30 dimensions, not 2.\n",
    "\n",
    "Therefore, we will need a way to reduce the dimensionality of our data, before we can perform clustering.  \n",
    "When finished, our ensemble model will look something like this:\n",
    "\n",
    " ![realistic data](images/ensembleModel.png)\n",
    "\n",
    "## Building Ensemble Model\n",
    "\n",
    "We must decided on a method for dimensionality reduction. We skip over the traditional PCA, and explore these two:\n",
    "+ TSNE\n",
    "+ UMAP\n",
    "\n",
    "We would like to select the method that creates the strongest delineation into two groups.   \n",
    "Here are two methods performed on the Train data:\n",
    "\n",
    "![TSNEvUMAP](images/TSNEvsUMAP.svg)\n",
    "\n",
    "Because we feel the UMAP method provides stronger delineation, we will proceed with this method. \n",
    "\n",
    "Next, we use the X, Y output of the UMAP to train a KMeans model, with 2 clusters. \n",
    "Here are the cluster predictions for the Train set:\n",
    "\n",
    "![TSNEvUMAP](images/clusteredUMAP.svg)\n",
    "\n",
    "On each of the clusters, we train a model to predict the data labels (\"M\" or \"B\").   \n",
    "Here is the confusion matrix for the cluster model predictions on the Train set:\n",
    "\n",
    "![cluster models predict train](images/clusterModelTrainCM.png)\n",
    "\n",
    "### HUGE PROBLEM: UMAP uses a stochastic process!\n",
    "\n",
    "UMAP uses a stochastic process for mapping its dimensionality reduction.   \n",
    "This means that the results will be different each time the function is run, even if the underlying data is the same!\n",
    "\n",
    "Ultimately, we cannot use the UMAP method for dimentionality reduction, because it is impossible to \"train\" a UMAP model that will reduce the dimensions of new data.\n",
    "\n",
    "### HUGE PROBLEM: workaround\n",
    "\n",
    "Instead, we will train a Neural Network work to mimic our UMAP results. \n",
    "\n",
    "First, we take our Train data set, and split it further into TrainL2, DevL2, and TestL2 sets, as show below:\n",
    "\n",
    "![sub data split](images/dataSubSplit.png)\n",
    "\n",
    "And we will build our model using the TrainL2 set. \n",
    "\n",
    "Many models were built, with many different hyperparameters for different number of layers, numer of neurons, activation functions, learning rates, optimization algorithms, and loss functions, \n",
    "\n",
    "For the sake of simplicity, we will summarize the results for 3 loss function:\n",
    "+ MSLE\n",
    "+ MAPE\n",
    "+ Huber\n",
    "\n",
    "as shown here:\n",
    "\n",
    "![sub data split](images/nnRedDims.png)\n",
    "\n",
    "The reason we selected these three loss functions for tested was because we are wanting to see clear deliniations in our reduced dimension.   \n",
    "MSLE is a standard to compare the rest to; it is a common loss function.   \n",
    "MAPE was chosen because it calculates losses bases on the relative error for a point, rather than the absolute error for a point. Essentially, an error of 100 on a datapoint with a value of 1000 would normally have a greater impace than an error of 10 on a datapoint with a value of 100, but with MAPE (mean absolute PERCENT error), both the erros are treated as 10%.    \n",
    "The Huber loss function is resistant to outliers. Many loss functions use the \"mean,\" and a mean is easily skewed by outliers. The Huber loss function tries to mitigate this.\n",
    "\n",
    "Here are what the Neural Nets predict for the TrainL2 and DevL2 sets. \n",
    "\n",
    "BLUE = ACTUAL   \n",
    "RED = PREDICTED\n",
    "![UMAP NN predictions](images/neurNetParamCompare.svg)\n",
    "\n",
    "Both the MAPE and Huber provide good delineation. \n",
    "\n",
    "We will make predictions for the DevL2 and TestL2, and use the KMeans on the predictions to predict cluster membership.    \n",
    "We then compare the predicted cluster membership to the true cluster memberships, and we will select the loss function that predicts the most correct memberships.\n",
    "\n",
    "The results on the DevL2 are:\n",
    "+ the KMeans model was able to predict 66 correct memberships with the MSLE loss function on the Dev set\n",
    "+ the KMeans model was able to predict 67 correct memberships with the MAPE loss function on the Dev set\n",
    "+ the KMeans model was able to predict 67 correct memberships with the Huber loss function on the Dev set\n",
    "+ Total observations in Dev set is: 70\n",
    "\n",
    "The results on the TestL2 are:\n",
    "+ the KMeans model was able to predict 54 correct memberships with the MSLE loss function on the Test set\n",
    "+ the KMeans model was able to predict 55 correct memberships with the MAPE loss function on the Test set\n",
    "+ the KMeans model was able to predict 56 correct memberships with the Huber loss function on the Test set\n",
    "+ Total observations in Test set is: 56\n",
    "\n",
    "Huber has the highest score, so we will use Huber. \n",
    "\n",
    "Finally, we train a Neural Network on the entire, original Train set, using the Huber loss function.    \n",
    "These are the parameters for the Dimensionality Reduction Neural Network:\n",
    "+ number of layers: 31\n",
    "+ number of neurons: L1-L30: 240; L31: 2\n",
    "+ activation functions: swish\n",
    "+ learning rate: 0.0001\n",
    "+ optimizer: Adam (with AMSGrad)\n",
    "+ epochs: 1,000\n",
    "\n",
    "We now have all of our models built, and we can move on. \n",
    "\n",
    "## Results \n",
    "\n",
    "Using our Ensemble model, we make label predictions on the Dev data set.\n",
    "Here are our results:\n",
    "\n",
    "![ensemble predict dev](images/ensembleDevResults.svg)\n",
    "\n",
    "The results are mediocre. Same False Negative rate as the Logistic Regression Model on the Dev set, but worse False Positive rate. \n",
    "\n",
    "Lets compare our Ensemble model to the Logistic Regression mode on the test set:\n",
    "\n",
    "![ensemble vs logReg](images/ensembleVsLogRegTestResults.svg)\n",
    "\n",
    "The results on the Test set look better for our Ensemble model, with a False Negative rate of 0%.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "I would not rely on this model. \n",
    "Here are the reasons:\n",
    "+ Does not give consistent results; any good results could be because of data split\n",
    "+ Does not significantly outperform a simple Logistic Regression Model (and is far more complex).\n",
    "+ Have less than 600 data points; NEED MORE DATA.\n",
    "\n",
    "Additionally, UMAP uses a stochastic method of reducing dimensionality. Because our data was normalized, we assumed it was homogeneous. If data is not homogeneous, any stochastic method would be greatly inappropriate (using a stochastic dimension reduction method in itself in inadvisable in general, if we are wanting to reduce the dimensions of new, unseen, data).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
